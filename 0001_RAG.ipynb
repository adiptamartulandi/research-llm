{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMAAwmrbZKMyED3O2lwTX9L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"D8_bUOGo3ptu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700649164217,"user_tz":-420,"elapsed":39733,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"3bfdff3a-bfa8-4ab5-ca8f-4367816f0c6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.5/220.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install huggingface_hub transformers datasets langchain openai chromadb sentencepiece ctransformers -q"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')"],"metadata":{"id":"HhjmWi6w4VPo","executionInfo":{"status":"ok","timestamp":1700649166425,"user_tz":-420,"elapsed":2213,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from langchain.llms import HuggingFaceHub\n","from langchain.embeddings import HuggingFaceHubEmbeddings\n","\n","from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n","\n","from langchain.chains import LLMChain, ConversationalRetrievalChain\n","from langchain.prompts import PromptTemplate"],"metadata":{"id":"yu89mBXg369U","executionInfo":{"status":"ok","timestamp":1700649167143,"user_tz":-420,"elapsed":722,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# LLM"],"metadata":{"id":"2fGRzHVY5RTr"}},{"cell_type":"code","source":["question = \"Who won the FIFA World Cup in the year 1994? \"\n","\n","template = \"\"\"\n","Question: {question}\n","Answer: \"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"],"metadata":{"id":"0wSaOcTF4NDZ","executionInfo":{"status":"ok","timestamp":1700649167143,"user_tz":-420,"elapsed":3,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## bigcode/starcoder"],"metadata":{"id":"GJPzF09bnmZo"}},{"cell_type":"code","source":["repo_id = \"Intel/neural-chat-7b-v3\""],"metadata":{"id":"6fqaAm3ennXU","executionInfo":{"status":"ok","timestamp":1700649805779,"user_tz":-420,"elapsed":3,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","\n","print(llm_chain.run(question))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"G2a8B8v-nqSx","executionInfo":{"status":"error","timestamp":1700650141003,"user_tz":-420,"elapsed":334687,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"1a27dbfa-d7d7-471b-9a35-8895424e4392"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-fcf335364272>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mllm_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    506\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             ]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    506\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     async def agenerate_prompt(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                 )\n\u001b[1;32m    655\u001b[0m             ]\n\u001b[0;32m--> 656\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             output = (\n\u001b[0;32m--> 531\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    532\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             text = (\n\u001b[0;32m-> 1053\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/huggingface_hub.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error raised by inference API: {response['error']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# Text generation return includes the starter text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error raised by inference API: Model Intel/neural-chat-7b-v3 time out"]}]},{"cell_type":"markdown","source":["## lmsys/fastchat-t5-3b-v1.0"],"metadata":{"id":"TvKuF6nVllSS"}},{"cell_type":"code","source":["repo_id = \"lmsys/fastchat-t5-3b-v1.0\""],"metadata":{"id":"s_BV39-mlmIV","executionInfo":{"status":"ok","timestamp":1700649169055,"user_tz":-420,"elapsed":10,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","\n","print(llm_chain.run(question))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmkhxLRTltK4","executionInfo":{"status":"ok","timestamp":1700649224241,"user_tz":-420,"elapsed":5755,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"67b3d7d9-c9e9-4174-ef58-981e80ff92b6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["<pad> Brazil\n"]}]},{"cell_type":"code","source":["llm_chain.run(\"halo apa kabar ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"tlNHn9JLl_rk","executionInfo":{"status":"ok","timestamp":1700649265562,"user_tz":-420,"elapsed":20122,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"c8b1bb78-7737-42f8-9314-c1958eb97a33"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<pad> Halo,  apa  kabar  yang  bisa  saya'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["llm_chain.run(\"whos win in the battle between gojo satoru vs ryomen sukuna ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"1uRbgiHrmIcx","executionInfo":{"status":"ok","timestamp":1700649296420,"user_tz":-420,"elapsed":4210,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"f20ebd4a-6f1a-4eb9-9e55-f326686e3e54"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<pad> Gojo  Satoru  defeated  Ryomen  Sukuna  in'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## ctransformers"],"metadata":{"id":"_bmTYj7Jsq2R"}},{"cell_type":"code","source":["from ctransformers import AutoModelForCausalLM\n","\n","llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGML\", model_file=\"llama-2-7b.ggmlv3.q2_K.bin\")\n","\n","print(llm(\"AI is going to\"))"],"metadata":{"id":"X4EsMSIRssWl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## flan by google"],"metadata":{"id":"nXSmzZ9e49wF"}},{"cell_type":"code","source":["repo_id = \"google/flan-t5-xxl\""],"metadata":{"id":"3DYbtzJd4vyk","executionInfo":{"status":"ok","timestamp":1700642617302,"user_tz":-420,"elapsed":2,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","\n","print(llm_chain.run(question))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQ8mJzFC41W_","executionInfo":{"status":"ok","timestamp":1700642618281,"user_tz":-420,"elapsed":460,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"70813ecd-d40f-401b-812d-a15bcfd21aac"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["brazil\n"]}]},{"cell_type":"markdown","source":["## dolly by databricks"],"metadata":{"id":"CD87yc5k4_E6"}},{"cell_type":"code","source":["repo_id = \"databricks/dolly-v2-3b\""],"metadata":{"id":"IzOjEUuA45gb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(question))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mUyADYnK5AfJ","executionInfo":{"status":"ok","timestamp":1700625375586,"user_tz":-420,"elapsed":938,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"2b3cabd0-13e0-4623-e62b-10e1b8ad2ed8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":[" First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.\n","\n","\n","Question: Who\n"]}]},{"cell_type":"markdown","source":["## falcon"],"metadata":{"id":"dX0D74el5Gte"}},{"cell_type":"code","source":["repo_id = \"tiiuae/falcon-40b\" #terlalu lama\n","repo_id_7b = \"tiiuae/falcon-7b-instruct\" #bisa cepet\n","repo_id_180b = \"tiiuae/falcon-180B-chat\" #pro account"],"metadata":{"id":"QwADAD185CRx","executionInfo":{"status":"ok","timestamp":1700642709300,"user_tz":-420,"elapsed":317,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id_7b\n",")"],"metadata":{"id":"hxa3ebXf5HuB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700642711178,"user_tz":-420,"elapsed":3,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"1223624d-bd4e-42e0-e0e5-6820f2be9314"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]}]},{"cell_type":"code","source":["llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(\"whos president of USA ?\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"AeVD36hNaSCn","executionInfo":{"status":"error","timestamp":1700642712171,"user_tz":-420,"elapsed":3,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"ffda8019-73d3-4cde-ba5d-888257d6e271"},"execution_count":59,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-77d16cf4443b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mllm_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whos president of USA ?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    506\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             ]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0mChain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \"\"\"\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         callback_manager = CallbackManager.configure(\n\u001b[1;32m    288\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mprep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0m_input_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    434\u001b[0m                     \u001b[0;34mf\"A single string input was passed in, but this chain expects \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;34mf\"multiple inputs ({_input_keys}). When a chain expects \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: A single string input was passed in, but this chain expects multiple inputs ({'chat_history', 'question'}). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`"]}]},{"cell_type":"code","source":["print(llm_chain.run(\"whos win in the battle between gojo satoru vs ryomen sukuna ?\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1COogiTZlf4","executionInfo":{"status":"ok","timestamp":1700629437934,"user_tz":-420,"elapsed":4644,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"1f0e9707-a9d3-4a0a-e762-e75c8351e296"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Gojo Satoru is a former member of the Jaganmaru, a group of elite assassins. He is a master of martial arts and has a unique ability to manipulate his surroundings. On the other hand, Ryomen Sukuna is a former member of the Jaganmaru and is considered one of the most powerful beings in the series. He has the ability to manipulate time and space and can even create black holes. In terms of power, it is difficult to determine a clear winner. However, Gojo Satoru's unique abilities and his experience in the Jaganmaru may give him an advantage over Sukuna.\n"]}]},{"cell_type":"markdown","source":["## llama"],"metadata":{"id":"d0MjD5SgMrjE"}},{"cell_type":"code","source":["repo_id = \"meta-llama/Llama-2-70b-chat-hf\" #need access"],"metadata":{"id":"PEIfhDg1MyNt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(question))"],"metadata":{"id":"SsqyL24pM0ka"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## code llama"],"metadata":{"id":"RR0EK03oNQZx"}},{"cell_type":"code","source":["repo_id = \"codellama/CodeLlama-34b-Instruct-hf\""],"metadata":{"id":"JcULoaCyNV3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(question))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aDfMEd90NXys","executionInfo":{"status":"ok","timestamp":1700626087533,"user_tz":-420,"elapsed":761,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"5e001f1a-f0fe-4fef-e6d8-003996d32801"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","1. Who won the FIFA World Cup in the year 1994?\n","\n"]}]},{"cell_type":"markdown","source":["## mistral"],"metadata":{"id":"QuzGmKe9NskM"}},{"cell_type":"code","source":["repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\""],"metadata":{"id":"klNWuc1KNw50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(question))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EntCcqg8Ny4X","executionInfo":{"status":"ok","timestamp":1700626120689,"user_tz":-420,"elapsed":1304,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"072490c1-06fa-4269-9c6c-2da0ffc56250"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","1. The year 1994 is a leap year.\n","2. The\n"]}]},{"cell_type":"markdown","source":["## orca"],"metadata":{"id":"6ZETXHkJN807"}},{"cell_type":"code","source":["repo_id_7b = \"microsoft/Orca-2-7b\" #lama sekali\n","repo_id = \"microsoft/Orca-2-13b\" #lama sekali"],"metadata":{"id":"YtPlnso-N90n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id_7b\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(question))"],"metadata":{"id":"xkDPPDqwN_d3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## yi"],"metadata":{"id":"zPw8vdVdODby"}},{"cell_type":"code","source":["repo_id = \"01-ai/Yi-34B\" #terlalu lama"],"metadata":{"id":"ksgXtie8OL9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(question))"],"metadata":{"id":"S0yrIk0QOOEZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## openchat"],"metadata":{"id":"i5iGvSuoOY-w"}},{"cell_type":"code","source":["repo_id = \"openchat/openchat_3.5\""],"metadata":{"id":"goDVAd5AOZ4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(question))"],"metadata":{"id":"LrBqKyGeOccG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## zephyr"],"metadata":{"id":"mSFCrob6Owre"}},{"cell_type":"code","source":["repo_id = \"HuggingFaceH4/zephyr-7b-beta\""],"metadata":{"id":"fKJGD1LnOyG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFaceHub(\n","    repo_id=repo_id\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","print(llm_chain.run(question))"],"metadata":{"id":"VqnwPF2uOzpU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# local pipeline"],"metadata":{"id":"EcSvTYa8ReWu"}},{"cell_type":"markdown","source":["## gpt 2"],"metadata":{"id":"MBvLoKUER1NN"}},{"cell_type":"code","source":["hf = HuggingFacePipeline.from_model_id(\n","    model_id=\"gpt2\",\n","    task=\"text-generation\"\n",")"],"metadata":{"id":"HId5tZKARfoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = prompt | hf\n","\n","print(chain.invoke({\"question\": question}))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCMOuEEdRlZ_","executionInfo":{"status":"ok","timestamp":1700627143303,"user_tz":-420,"elapsed":2179,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"3fba8069-7fc1-4077-9891-8e12405b1a04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","The FIFA World Cup in 1994, which took place in the year 1995, was a\n"]}]},{"cell_type":"markdown","source":["## falcon"],"metadata":{"id":"wEzLLIVfR4Ru"}},{"cell_type":"code","source":["#file terlalu besar\n","hf = HuggingFacePipeline.from_model_id(\n","    model_id=\"tiiuae/falcon-40b\",\n","    task=\"text-generation\"\n",")"],"metadata":{"id":"UJoh_6DaR6AN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = prompt | hf\n","\n","print(chain.invoke({\"question\": question}))"],"metadata":{"id":"r99eea0ySA5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## orca 2"],"metadata":{"id":"fE-y_gY9ToK5"}},{"cell_type":"code","source":["#crash\n","hf = HuggingFacePipeline.from_model_id(\n","    model_id=\"microsoft/Orca-2-7b\",\n","    task=\"text-generation\"\n",")"],"metadata":{"id":"j3RiMzanTp3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = prompt | hf\n","\n","print(chain.invoke({\"question\": question}))"],"metadata":{"id":"DsvyySdCTpx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## openchat"],"metadata":{"id":"KDgbt4xNUVlY"}},{"cell_type":"code","source":["#crash\n","hf = HuggingFacePipeline.from_model_id(\n","    model_id=\"openchat/openchat_3.5\",\n","    task=\"text-generation\"\n",")"],"metadata":{"id":"mo2b0aDyUcLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = prompt | hf\n","\n","print(chain.invoke({\"question\": question}))"],"metadata":{"id":"yBQ_cJtxUcG4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## zephyr"],"metadata":{"id":"RgIheWETYYgA"}},{"cell_type":"code","source":["hf = HuggingFacePipeline.from_model_id(\n","    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n","    task=\"text-generation\"\n",")"],"metadata":{"id":"dBU-u3qEYYUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = prompt | hf\n","\n","print(chain.invoke({\"question\": question}))"],"metadata":{"id":"wzaSi59AYYSL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Embeddings"],"metadata":{"id":"Rwjkrc8i5bzN"}},{"cell_type":"code","source":["embeddings = HuggingFaceHubEmbeddings()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4U5HLxZ5LVe","executionInfo":{"status":"ok","timestamp":1700642630005,"user_tz":-420,"elapsed":627,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"efbb4f30-5f68-4b0b-ed33-2d60a45ea5c5"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n","You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n","WARNING:huggingface_hub.inference_api:You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"]}]},{"cell_type":"code","source":["text = \"This is a test document.\""],"metadata":{"id":"AUWnYlcb5y_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_result = embeddings.embed_query(text)"],"metadata":{"id":"URpwRMGlIx9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_result"],"metadata":{"id":"ZQPk2aj9I7Nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc_result = embeddings.embed_documents([text])\n","doc_result"],"metadata":{"id":"3xCN8pa5I8yq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"ReLH-_K0Jj9A"}},{"cell_type":"code","source":["from langchain.chains import LLMChain\n","from langchain.prompts import (\n","    ChatPromptTemplate,\n","    HumanMessagePromptTemplate,\n","    MessagesPlaceholder,\n","    SystemMessagePromptTemplate,\n",")\n","from langchain.memory import ConversationBufferMemory\n","\n","# Prompt\n","prompt = ChatPromptTemplate(\n","    messages=[\n","        SystemMessagePromptTemplate.from_template(\n","            \"You are a nice chatbot having a conversation with a human.\"\n","        ),\n","        # The `variable_name` here is what must align with memory\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        HumanMessagePromptTemplate.from_template(\"{question}\"),\n","    ]\n",")\n","\n","# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n","# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n","memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n","conversation = LLMChain(llm=llm, prompt=prompt, verbose=False, memory=memory)"],"metadata":{"id":"CyPpH9SkJABg","executionInfo":{"status":"ok","timestamp":1700642632766,"user_tz":-420,"elapsed":2,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n","conversation({\"question\": \"hi\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMv8mq9xJuza","executionInfo":{"status":"ok","timestamp":1700642633088,"user_tz":-420,"elapsed":6,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"56894e12-ce57-48b8-8a99-854f2b28f49f"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'hi',\n"," 'chat_history': [HumanMessage(content='hi'),\n","  AIMessage(content='System: hi, how are you?')],\n"," 'text': 'System: hi, how are you?'}"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["conversation({\"question\": \"what is NLP ?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8OctDHx3Jvgc","executionInfo":{"status":"ok","timestamp":1700642368932,"user_tz":-420,"elapsed":513,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"47c55374-ed64-498d-9a27-93f9a5a3985e"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'what is NLP ?',\n"," 'chat_history': [HumanMessage(content='hi'),\n","  AIMessage(content='System: hi, how are you?'),\n","  HumanMessage(content='what is NLP ?'),\n","  AIMessage(content='System: Natural Language Processing is the study of how language works.')],\n"," 'text': 'System: Natural Language Processing is the study of how language works.'}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["conversation({\"question\": \"whos win in the battle between gojo satoru vs ryomen sukuna ?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"loIJxX_oJ0IT","executionInfo":{"status":"ok","timestamp":1700642394012,"user_tz":-420,"elapsed":1389,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"d29c41e8-3e9d-4001-c79c-a1e4775cbc02"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'whos win in the battle between gojo satoru vs ryomen sukuna ?',\n"," 'chat_history': [HumanMessage(content='hi'),\n","  AIMessage(content='System: hi, how are you?'),\n","  HumanMessage(content='what is NLP ?'),\n","  AIMessage(content='System: Natural Language Processing is the study of how language works.'),\n","  HumanMessage(content='whos win in the battle between gojo satoru vs ryomen sukuna ?'),\n","  AIMessage(content='AI: System: Gojo Satoru won the battle.')],\n"," 'text': 'AI: System: Gojo Satoru won the battle.'}"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# RAG"],"metadata":{"id":"ZV-qzjdQJ_Ct"}},{"cell_type":"code","source":["from langchain.document_loaders import WebBaseLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma"],"metadata":{"id":"BpuF6DzhKDjz","executionInfo":{"status":"ok","timestamp":1700642531077,"user_tz":-420,"elapsed":1,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["loader = WebBaseLoader(\"https://beebom.com/jujutsu-kaisen-gojo-vs-sukuna/\")\n","data = loader.load()"],"metadata":{"id":"HR_Srh8MJ6mU","executionInfo":{"status":"ok","timestamp":1700642639580,"user_tz":-420,"elapsed":1818,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n","all_splits = text_splitter.split_documents(data)\n","\n","vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"],"metadata":{"id":"DS4AltvDKEwB","executionInfo":{"status":"ok","timestamp":1700642689432,"user_tz":-420,"elapsed":49853,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["retriever = vectorstore.as_retriever()\n","qa_retriever = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"],"metadata":{"id":"y_CsGc_4KMaD","executionInfo":{"status":"ok","timestamp":1700642689433,"user_tz":-420,"elapsed":16,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["qa_retriever({\"question\": \"whos win in the battle between gojo satoru vs ryomen sukuna ?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-HuOtWxKtdM","executionInfo":{"status":"ok","timestamp":1700642691057,"user_tz":-420,"elapsed":1638,"user":{"displayName":"Adipta Martulandi","userId":"03806677004074641623"}},"outputId":"dcb471ab-17c6-4eee-8e83-3e530beb31bf"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'whos win in the battle between gojo satoru vs ryomen sukuna ?',\n"," 'chat_history': [HumanMessage(content='hi'),\n","  AIMessage(content='System: hi, how are you?'),\n","  HumanMessage(content='whos win in the battle between gojo satoru vs ryomen sukuna ?'),\n","  AIMessage(content='Gojo Satoru')],\n"," 'answer': 'Gojo Satoru'}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":[],"metadata":{"id":"7zfY75wzKvTA"},"execution_count":null,"outputs":[]}]}